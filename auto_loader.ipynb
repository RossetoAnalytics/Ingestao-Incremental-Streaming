{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8cb5091-5f78-4909-9bef-3172cbf2546e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ## Carga Incremental com Auto Loader na camada Bronze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fd3acf-4b0d-46cf-968f-fc89f554deb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "O Auto Loader é um recurso do Apache Spark que facilita a ingestão incremental de dados em tempo real. Ele permite a leitura automática de novos arquivos em um diretório monitorado, aplicando um esquema definido e gerenciando checkpoints para garantir a consistência dos dados. \n",
    "\n",
    "Com o Auto Loader, os dados são ingeridos de forma atômica, garantindo que não haja perda de informações em caso de falhas no processo de carregamento. Essa abordagem é especialmente útil para cenários de big data, onde grandes volumes de dados são gerados constantemente, proporcionando uma solução eficiente e resiliente para a ingestão de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840d272b-a6e2-4708-a472-87af67ce7734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"dbfs:/mnt/checkpoints/bronze\"\n",
    "schema_path = \"dbfs:/mnt/checkpoints/bronze_schema\"\n",
    "source_path = \"dbfs:/mnt/daily_data/\"  # Caminho de origem dos arquivos JSON\n",
    "\n",
    "def process_bronze():\n",
    "    query = (spark.readStream\n",
    "                  .format(\"cloudFiles\")  # Utiliza Auto Loader para ingestão eficiente de dados\n",
    "                  .option(\"cloudFiles.format\", \"json\") # Especifica que os dados de origem estão no formato JSON\n",
    "                  .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "                  .load(source_path) # Diretório de origem contendo os arquivos de dados diários\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", checkpoint_path)  # Localização do checkpoint para processamento com estado\n",
    "                  .trigger(availableNow=True) # Dispara a consulta para processar todos os dados disponíveis e terminar\n",
    "                  # pode ser usado diferentes parâmetros, como \"processingTime\", com o qual você define um intervalo de tempo para os dados serem processados novamente na tabela Bronze.\n",
    "                  .table(\"bronze\"))\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2faa11-618b-4ef7-b445-64a625f43b45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_bronze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cbed2a4-369d-4e82-84fb-825ff89f3b49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "abaixo, a consulta é feita da tabela gerada, você pode repetir a consulta assim que fizer a ingestão de novos dados com o gerador de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a438d03-cb1a-4863-bd34-96e77acea1ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+--------------------+-------------+\n|price|    product|quantity|           sale_date|_rescued_data|\n+-----+-----------+--------+--------------------+-------------+\n|40.26| Product_49|       5|2024-09-12T20:32:...|         NULL|\n|98.56| Product_92|       9|2024-10-02T20:32:...|         NULL|\n|53.93| Product_23|       4|2024-09-12T20:32:...|         NULL|\n|78.76| Product_90|       9|2024-09-14T20:32:...|         NULL|\n|20.55| Product_83|       7|2024-10-11T20:32:...|         NULL|\n|47.49| Product_73|       4|2024-09-25T20:32:...|         NULL|\n|92.56| Product_65|       7|2024-09-29T20:32:...|         NULL|\n|14.79|  Product_6|       3|2024-10-05T20:32:...|         NULL|\n|88.42| Product_52|       9|2024-09-24T20:32:...|         NULL|\n|15.56| Product_23|       6|2024-09-11T20:32:...|         NULL|\n|79.61| Product_68|       5|2024-10-07T20:32:...|         NULL|\n|42.42|Product_100|       6|2024-09-26T20:32:...|         NULL|\n|98.07| Product_41|       3|2024-10-11T20:32:...|         NULL|\n|83.61| Product_56|       9|2024-10-07T20:32:...|         NULL|\n|71.18| Product_33|       2|2024-09-25T20:32:...|         NULL|\n|25.53| Product_40|       1|2024-10-10T20:32:...|         NULL|\n|94.25| Product_28|       3|2024-09-23T20:32:...|         NULL|\n| 11.3| Product_70|       8|2024-10-02T20:32:...|         NULL|\n|52.43|Product_100|       5|2024-09-20T20:32:...|         NULL|\n|10.11| Product_65|       6|2024-10-06T20:32:...|         NULL|\n+-----+-----------+--------+--------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Consultar a tabela bronze para ver os dados processados\n",
    "bronze_df = spark.table(\"bronze\")\n",
    "bronze_df.show()  # Mostrar as primeiras linhas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a027d2-21ed-43dc-9819-587e206eabbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bronze_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aac0728-d762-4def-bcf9-2ae181a281c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Processamento em batch da camada bronze para silver usando processamento incremental com Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc79a11c-0ea0-4c8f-8741-4b2e3010bc90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "    \n",
    "def update_silver():\n",
    "    query = (spark.readStream\n",
    "                  .table(\"bronze\")  # Leitura da tabela Bronze em modo de streaming\n",
    "                  .withColumn(\"processed_time\", F.current_timestamp())  # Adiciona uma coluna de timestamp\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", \"dbfs:/mnt/checkpoints/silver\")  # Localização do checkpoint para a tabela Silver\n",
    "                  .trigger(availableNow=True)  # Processa todos os dados disponíveis e, em seguida, encerra\n",
    "                  .table(\"silver\"))  # Grava os dados processados na tabela Silver\n",
    "    \n",
    "    query.awaitTermination()  # Aguarda a finalização da consulta antes de encerrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b549743-f109-489b-a7c6-5462359601af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#chamando a função\n",
    "update_silver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ddd08a1-7dcc-469e-94f0-729e82c32d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>6000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizando o count na Silver para verificar a carga incremental:\n",
    "spark.sql(\"\"\"\n",
    "select count(1)\n",
    "from silver\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "auto_loader",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
